# -*- coding: utf-8 -*-
"""054_Assignment_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6dNg7y9uw0YQ0TFtWCFNEY8GXwGaDjX

Name: Pranoti Musmade

PRN: 123B1B054

Title : Applying regularization techniques, cross validation, and gradient descent techiniques on dataset.
"""

import pandas as pd
df = pd.read_csv("/content/cleaned_dataset_insurance.csv")
df.head(2)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

X = df.drop(columns=['charges'])
y = df['charges']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Linear Regression
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# Scores
train_score = r2_score(y_train, y_train_pred)
test_score = r2_score(y_test, y_test_pred)


# Evaluation
print("Linear Regression Results:")
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

print(f"Train R²: {train_score:.3f}")
print(f"Test R²: {test_score:.3f}")

# Decision based on scores
if train_score - test_score > 0.1:
    print("High variance detected → Apply regularization (Ridge/Lasso/ElasticNet).")
elif train_score < 0.5 and test_score < 0.5:
    print("High bias detected → Regularization won't help. Add features or use a complex model.")
else:
    print("Model is well-balanced → Regularization may not be necessary.")

"""1. Train Linear Regression
  
2. Evaluate Train R² and Test R²

*   Is (Train R² >> Test R²)?

      *   YES → High Variance → Apply Regularization (Ridge/Lasso/ElasticNet)
      * NO

*   Are both R² scores low (< 0.5)?
      *  YES → High Bias → Add features, use polynomial terms or other models
      *  NO Model is good → Regularization not necessary

If the above check indicates high variance, proceed with:

* Ridge (if multicollinearity is the main issue)

* Lasso (if you want feature selection)

* ElasticNet (balanced approach)

1.Ridge:

**Concept**

* Ridge regression adds an L2 penalty term to the loss function (sum of squared coefficients).

* It shrinks coefficients towards zero but never makes them exactly zero.

* Best for handling multicollinearity (when features are highly correlated).


**Mathematical Form**


Loss = ∑(𝑦−𝑦)^2 + 𝛼∑𝑤^2

α = regularization strength (higher = more shrinkage)

w = model coefficients

**Advantages**

  * Reduces model complexity and variance.

  * Prevents overfitting.

  * Works well when all features are important.

**Limitations**

  * Does not perform feature selection (keeps all features).

**Alpha:**

* Low alpha (e.g., 0.01) → Less regularization, model closer to linear regression.

* High alpha (e.g., 10 or 100) → Stronger regularization, coefficients shrink more.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Features and target
X = df.drop(columns=['charges'])
y = df['charges']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ridge Regression model
ridge = Ridge(alpha=1.0)  # Try values like 0.1, 1, 10 for alpha
ridge.fit(X_train_scaled, y_train)

# Predictions
y_pred = ridge.predict(X_test_scaled)

# Evaluation
print("Ridge Regression Results:")
print("R² Score:", r2_score(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("Coefficients:", ridge.coef_)
print("Intercept:", ridge.intercept_)

"""**2.Lasso Regression (L1 Regularization)**

**Concept**

* Lasso adds an L1 penalty term (sum of absolute values of coefficients).

* It can shrink some coefficients to exactly zero, effectively performing feature selection.

**Mathematical Form**

    Loss=∑(𝑦−𝑦^)2+𝛼∑∣𝑤∣

**Advantages**

* Performs automatic feature selection.

* Useful when you suspect that only a few features are important.

**Limitations**

* May struggle with datasets where features are highly correlated (picks one and ignores others).

* Too high alpha can remove too many features.

**How to Tune alpha**

* Lasso can set some coefficients to zero → useful for feature selection.

* If many coefficients become zero, try lowering alpha.

* If almost no coefficients are zero, try increasing alpha.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Features and target
X = df.drop(columns=['charges'])
y = df['charges']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Lasso Regression model
lasso = Lasso(alpha=0.01)  # Try values like 0.001, 0.01, 0.1 for alpha
lasso.fit(X_train_scaled, y_train)

# Predictions
y_pred = lasso.predict(X_test_scaled)

# Evaluation
print("Lasso Regression Results:")
print("R² Score:", r2_score(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("Coefficients:", lasso.coef_)
print("Intercept:", lasso.intercept_)

"""**3.ElasticNet Regression (L1 + L2 Regularization)**

**Concept**

ElasticNet combines both L1 (Lasso) and L2 (Ridge) penalties.

Controlled by two parameters:

α → overall strength

l1_ratio → balance between Lasso (1.0) and Ridge (0.0)

**Mathematical Form**


Loss = ∑(𝑦−𝑦^)2+𝛼(𝑙_𝑟𝑎𝑖𝑜∑∣𝑤+(1−𝑙1_𝑟𝑎𝑡𝑖𝑜)∑𝑤^2)

**Advantages**

* Balances feature selection (Lasso) and coefficient shrinkage (Ridge).

* Works well with datasets that have correlated features and sparse features.

**Limitations**

Requires tuning of two parameters (α and l1_ratio).

**Notes:**

* alpha → Controls overall regularization strength.

* l1_ratio → Balances Lasso and Ridge:

* 0 → behaves like Ridge

* 1 → behaves like Lasso

* Between 0–1 → hybrid

* Useful when dataset has both correlated features and some irrelevant ones.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Features and target
X = df.drop(columns=['charges'])
y = df['charges']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ElasticNet Regression model
elastic = ElasticNet(alpha=0.01, l1_ratio=0.5)  # l1_ratio: 0 = Ridge, 1 = Lasso
elastic.fit(X_train_scaled, y_train)

# Predictions
y_pred = elastic.predict(X_test_scaled)

# Evaluation
print("ElasticNet Regression Results:")
print("R² Score:", r2_score(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("Coefficients:", elastic.coef_)
print("Intercept:", elastic.intercept_)

"""| Feature                             | **Ridge Regression (L2)**                                 | **Lasso Regression (L1)**                          | **ElasticNet Regression (L1 + L2)**            |
| ----------------------------------- | --------------------------------------------------------- | -------------------------------------------------- | ---------------------------------------------- |
| **Effect on coefficients**          | Shrinks coefficients (never zero)                         | Shrinks some coefficients to exactly zero          | Some coefficients shrink, some may become zero |
| **Feature selection**               |  No                                                      | Yes (automatically removes irrelevant features)  | Partial (depends on `l1_ratio`)                |
| **Best for**                        | Multicollinearity & when all features matter              | Sparse datasets & when feature selection is needed | Mixed datasets (correlated + sparse features)  |
| **Alpha (regularization strength)** | Higher → stronger penalty → less overfitting              | Higher → more coefficients removed                 | Controlled by `alpha` + `l1_ratio`             |
| **When to use?**                    | When you want to keep all predictors but control variance | When you want simpler model with fewer predictors  | When you want balance between Ridge & Lasso    |

**Self Study**

**Cross validation**

Cross-validation is a technique used to evaluate how well your machine learning model will generalize to unseen data. Instead of splitting the data once into train and test sets, cross-validation repeatedly splits the data into multiple parts to ensure more reliable performance estimation.

**What is Cross-Validation?**

Problem with single train-test split:

* Your model’s performance may depend on how the data was split.

*  A single split may lead to overestimating or underestimating the model's accuracy.

**Solution – Cross-validation:**

* Divide the dataset into multiple folds (usually 5 or 10).

* train the model on k-1 folds and test it on the remaining fold.

* Repeat this process k times, each time using a different fold as the test set.

* Average the results to get a more stable performance metric.

**Types of Cross-Validation**

* K-Fold Cross-Validation – Most common (e.g., 5-fold, 10-fold).

* Stratified K-Fold – Keeps class proportions same in each fold (used in classification).

* Leave-One-Out (LOOCV) – Each data point becomes a test set once.

* Repeated K-Fold – Runs K-Fold multiple times for more stable results.

**When Should You Use Cross-Validation?**

* When you have limited data and want to avoid wasting it by only doing one split.

* When you suspect variance in your model’s performance across different splits.

* Before choosing regularization parameters (alpha for Ridge/Lasso) – it helps in hyperparameter tuning.
"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, make_scorer
import numpy as np

# Example model
model = Ridge(alpha=1.0)

# Perform 5-fold cross-validation using Negative Mean Squared Error
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

# Convert negative MSE to positive
mse_scores = -scores
mean_mse = np.mean(mse_scores)

print("MSE for each fold:", mse_scores)
print("Average MSE:", mean_mse)

"""To choose the best regularization parameter (alpha) for Ridge and Lasso, you can use cross-validation with hyperparameter tuning. The most common approach is Grid Search with Cross-Validation (GridSearchCV)."""

from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, make_scorer
import numpy as np

# Suppose your features and target
# X = your feature matrix
# y = your target column

# Define the alpha values to test
alpha_values = [0.01, 0.1, 1, 10, 100]

# Ridge Regression
ridge = Ridge()
ridge_params = {'alpha': alpha_values}

ridge_grid = GridSearchCV(ridge,
                          param_grid=ridge_params,
                          scoring='neg_mean_squared_error',
                          cv=5)  # 5-fold cross-validation
ridge_grid.fit(X, y)

best_ridge_alpha = ridge_grid.best_params_['alpha']
ridge_best_model = ridge_grid.best_estimator_

print("Best Ridge Alpha:", best_ridge_alpha)
print("Best Ridge MSE:", -ridge_grid.best_score_)

# Lasso Regression
lasso = Lasso(max_iter=10000)  # high max_iter to ensure convergence
lasso_params = {'alpha': alpha_values}

lasso_grid = GridSearchCV(lasso,
                          param_grid=lasso_params,
                          scoring='neg_mean_squared_error',
                          cv=5)
lasso_grid.fit(X, y)

best_lasso_alpha = lasso_grid.best_params_['alpha']
lasso_best_model = lasso_grid.best_estimator_

print("Best Lasso Alpha:", best_lasso_alpha)
print("Best Lasso MSE:", -lasso_grid.best_score_)

"""**What is Gradient Descent?**

Gradient Descent is an optimization algorithm used to minimize a loss (or cost) function by iteratively adjusting the model parameters (weights and bias). It is widely used in Linear Regression, Logistic Regression, Neural Networks, and Deep Learning.

**Key Idea**

* Start with some initial parameters (random or zero).

* Compute the gradient (slope) of the loss function with respect to the parameters.

* Update the parameters in the opposite direction of the gradient to reduce the loss.

Update Rule:


θ = θ − α ⋅ ∂J(θ)/∂θ
	​

Where:

θ = parameters (weights)

α = learning rate (step size)

J(θ) = cost function (e.g., Mean Squared Error)

**Types of Gradient Descent**

1. Batch Gradient Descent – Uses the whole dataset to compute gradients.

2. Stochastic Gradient Descent (SGD) – Updates parameters for each sample.

3. Mini-Batch Gradient Descent – Uses a small subset (batch) of data.
"""

import pandas as pd
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Features (X) and target (y)
X = df.drop(columns=['charges'])  # all except charges
y = df['charges']

# Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Gradient Descent Regression
model = SGDRegressor(max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42)

# Train model
model.fit(X_train_scaled, y_train)

# Predictions
y_pred = model.predict(X_test_scaled)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
print("Weights (coefficients):", model.coef_)
print("Intercept:", model.intercept_)
print("Mean Squared Error:", mse)

"""We can also use gradient descent with Ridge/LAssso

eg. model = SGDRegressor(penalty='l2', alpha=0.001)  # Ridge with gradient descent

penalty='l2' → Ridge

penalty='l1' → Lasso

penalty='elasticnet' → ElasticNet (also set l1_ratio=0.5 or another value)

**Conclusion**

Linear Regression was used to predict insurance charges, but it showed signs of overfitting. To improve generalization, Ridge, Lasso, and ElasticNet regularizations were applied. Ridge controlled coefficient magnitude, Lasso performed feature selection, and ElasticNet balanced both. Regularization resulted in better model stability and reduced variance compared to plain Linear Regression.
"""